{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNPOD_ENDPOINT_ID = os.getenv(\"RUNPOD_CHATBOT_URL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNPOD_EMBEDDING_KEY = os.getenv(\"RUNPOD_EMBEDDING_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to Endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"RUNPOD_TOKEN\"),\n",
    "    base_url = f\"https://api.runpod.ai/v2/{RUNPOD_ENDPOINT_ID}/openai/v1\",\n",
    ")\n",
    "\n",
    "model_name = os.getenv(\"MODEL_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.OpenAI at 0x280db177160>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model = model_name,\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what is the capital of germany?\"\n",
    "        }\n",
    "    ],\n",
    "    temperature = 0.0,\n",
    "    top_p = 0.8,\n",
    "    max_tokens= 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9a6ee438e98844e38b6c564a6ecdcd16', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Germany is Berlin.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=[]), stop_reason=None)], created=1741052738, model='meta-llama/Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=8, prompt_tokens=42, total_tokens=50, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Germany is Berlin.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for chatbot response\n",
    "- append messages based on role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chatbot_response(client, model_name, messages, temperature = 0.0):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        client (str): openai client\n",
    "        model_name (str): huggingface model name\n",
    "        messages (str): input messages\n",
    "        temperature (float, optional): creativity of the model. Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    input_messages = []\n",
    "    for message in messages:\n",
    "        input_messages.append({\n",
    "            \"role\": message[\"role\"],\n",
    "            \"content\": message[\"content\"]\n",
    "        })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        \n",
    "        model = model_name,\n",
    "        messages = input_messages,\n",
    "        temperature = temperature,\n",
    "        top_p = 0.8,\n",
    "        max_tokens= 2000\n",
    "    ).choices[0].message.content   \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of the United States of America (USA) is Washington, D.C. (short for District of Columbia).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"what is the capital of USA\"\n",
    "    }\n",
    "]\n",
    "\n",
    "get_chatbot_response(client, model_name, messages, temperature = 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"country\": \"India\",\n",
      "        \"capital\": \"New Delhi\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"\"\"You're a helpful assistant that answers questions about capitals of countries.\n",
    "Your output should be in a structured JSON format exactly like the one below. \n",
    "You are not allowed to write anything other than the JSON object:\n",
    "[\n",
    "    {\n",
    "        \n",
    "        \"country\": the country that you will get the capital of,\n",
    "        \"capital\": the capital of the country mentioned,\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"what is the capital of India?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_chatbot_response(client, model_name, messages, temperature = 0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'country': 'India', 'capital': 'New Delhi'}]\n"
     ]
    }
   ],
   "source": [
    "json_response = json.loads(response)\n",
    "print(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(json_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Input Structuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"country\": \"India\",\n",
      "        \"capital\": \"New Delhi\"\n",
      "    },\n",
      "    {\n",
      "        \"country\": \"USA\",\n",
      "        \"capital\": \"Washington, D.C.\"\n",
      "    },\n",
      "    {\n",
      "        \"country\": \"Italy\",\n",
      "        \"capital\": \"Rome\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "Get me the capitals of the following countries:\n",
    "'''\n",
    "1. India\n",
    "2. USA\n",
    "3. Italy\n",
    "'''\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "messages.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": user_input\n",
    "})\n",
    "\n",
    "\n",
    "response = get_chatbot_response(client, model_name, messages, temperature = 0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'country': 'India', 'capital': 'New Delhi'},\n",
       " {'country': 'USA', 'capital': 'Washington, D.C.'},\n",
       " {'country': 'Italy', 'capital': 'Rome'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json format\n",
    "json_response = json.loads(response)\n",
    "json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Chain of Thought Prompting\n",
    "- Give the model time to think step by step\n",
    "- best method out of all\n",
    "- helps in complex reasoning\n",
    "- reference: https://arxiv.org/abs/2205.11916"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"result\": 6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Calculate the result of this equation: 1+ 5\n",
    "\n",
    "your output should be in a structured JSON format exactly like the one below. \n",
    "You are not allowed to write anything other than the JSON object:\n",
    "{\n",
    "    \"result\": the final result of the equation\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_chatbot_response(client, model_name, messages, temperature = 0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving a more complex task to the model and asking it to break it down into smaller steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": -234\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Calculate the result of this equation: 234/2*8765+90876*11-98654\n",
    "\n",
    "your output should be in a structured JSON format exactly like the one below. \n",
    "You are not allowed to write anything other than the JSON object:\n",
    "{\n",
    "    \"result\": the final result of the equation\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_chatbot_response(client, model_name, messages, temperature = 0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1926487.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "234/2*8765+90876*11-98654"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the actual output is different that what the model generated. Hence use the chain of thought prompting to get the model to think step by step and then generate the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"steps\": \"First, we need to follow the BODMAS order of operations. \n",
      "    1. Divide 234 by 2: 234 / 2 = 117\n",
      "    2. Multiply 117 by 8765: 117 * 8765 = 1025125\n",
      "    3. Multiply 90876 by 11: 90876 * 11 = 998556\n",
      "    4. Add 1025125 and 998556: 1025125 + 998556 = 2023681\n",
      "    5. Subtract 98654 from 2023681: 2023681 - 98654 = 1928127\",\n",
      "    \"result\": 1928127\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"\"\"\n",
    "Calculate the result of this equation: 234/2*8765+90876*11-98654\n",
    "\n",
    "your output should be in a structured JSON format exactly like the one below. \n",
    "You are not allowed to write anything other than the JSON object:\n",
    "{\n",
    "    \"steps\": This is where you solve the equation step by step following the BODMAS order of operations.\n",
    "    You need to show your work and calculate each step leading to the final result. Feel free to write in free text\n",
    "    \n",
    "    \"result\": the final result of the equation\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    }\n",
    "]\n",
    "\n",
    "response = get_chatbot_response(client, model_name, messages, temperature = 0.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output is way better than the previous one. The model is able to break down the problem into smaller steps and then generate the final output. This is a great example of how chain of thought prompting can be used to improve the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_client = OpenAI(\n",
    "    api_key=os.getenv(\"RUNPOD_TOKEN\"),\n",
    "    base_url = f\"https://api.runpod.ai/v2/{RUNPOD_EMBEDDING_KEY}/openai/v1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(embedding_client, text_input, model_name):\n",
    "    \"\"\"returns the embedding of the input text using the specified model.\n",
    "\n",
    "    Args:\n",
    "        embedding_client (str): openai client\n",
    "        text_input (str): input text\n",
    "        model_name (str): huggingface model name\n",
    "    Returns:\n",
    "        str: _description_\n",
    "    \"\"\"\n",
    "    \n",
    "    output = embedding_client.embeddings.create(\n",
    "        input = text_input,\n",
    "        model=model_name\n",
    "    )\n",
    "    \n",
    "    embeddings_lst = []\n",
    "    for embedding_object in output.data:\n",
    "        embeddings_lst.append(embedding_object.embedding)\n",
    "    \n",
    "    return embeddings_lst\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"get me the embeddings of this text: 'hello world\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'meta-llama/Llama-3.1-8B-Instruct'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.0274015124887228,\n",
       "  -0.012045375071465969,\n",
       "  0.01661483757197857,\n",
       "  -0.04113278165459633,\n",
       "  0.022015802562236786,\n",
       "  -0.004413077607750893,\n",
       "  -0.05349092558026314,\n",
       "  0.020566390827298164,\n",
       "  0.024182291701436043,\n",
       "  -0.0031963344663381577,\n",
       "  -0.036189526319503784,\n",
       "  0.004493176471441984,\n",
       "  0.02430434711277485,\n",
       "  -0.014471232891082764,\n",
       "  0.037623681128025055,\n",
       "  0.02778293564915657,\n",
       "  -0.005206439644098282,\n",
       "  0.02171066403388977,\n",
       "  -0.08043472468852997,\n",
       "  0.0010489164851605892,\n",
       "  0.1196146160364151,\n",
       "  0.025494391098618507,\n",
       "  -0.0031066997908055782,\n",
       "  -0.03609798476099968,\n",
       "  -0.026699692010879517,\n",
       "  0.02659289352595806,\n",
       "  -0.017270885407924652,\n",
       "  0.006053201388567686,\n",
       "  0.017987962812185287,\n",
       "  -0.11857714504003525,\n",
       "  -0.022976992651820183,\n",
       "  -0.04546575993299484,\n",
       "  0.05114135146141052,\n",
       "  0.0003101455222349614,\n",
       "  0.061333004385232925,\n",
       "  -0.001341659459285438,\n",
       "  -0.04421468824148178,\n",
       "  0.0213139820843935,\n",
       "  -0.03001045249402523,\n",
       "  0.041041240096092224,\n",
       "  0.03948502987623215,\n",
       "  -0.038844235241413116,\n",
       "  -0.0007342415046878159,\n",
       "  0.0032592695206403732,\n",
       "  0.010595963336527348,\n",
       "  -0.10020775347948074,\n",
       "  -0.030117252841591835,\n",
       "  0.00739962887018919,\n",
       "  -0.02392292395234108,\n",
       "  -0.00954323261976242,\n",
       "  -0.002849238459020853,\n",
       "  0.004630489274859428,\n",
       "  -0.021756434813141823,\n",
       "  -0.005095826927572489,\n",
       "  0.08842937648296356,\n",
       "  0.04699145630002022,\n",
       "  0.06517776101827621,\n",
       "  0.028881438076496124,\n",
       "  -0.005767133552581072,\n",
       "  0.035792842507362366,\n",
       "  -0.022168371826410294,\n",
       "  0.019513661041855812,\n",
       "  -0.10966707766056061,\n",
       "  0.09611888974905014,\n",
       "  -0.008993981406092644,\n",
       "  0.007548384368419647,\n",
       "  -0.022000545635819435,\n",
       "  0.038844235241413116,\n",
       "  0.013006564229726791,\n",
       "  0.040278393775224686,\n",
       "  -0.008940582163631916,\n",
       "  -0.04909691959619522,\n",
       "  0.004558018874377012,\n",
       "  0.06108889356255531,\n",
       "  -0.005797647405415773,\n",
       "  0.013151505030691624,\n",
       "  0.029049264267086983,\n",
       "  -0.025006167590618134,\n",
       "  0.004176594782620668,\n",
       "  -0.038020361214876175,\n",
       "  -0.004443591460585594,\n",
       "  0.02496039681136608,\n",
       "  0.046564262360334396,\n",
       "  0.004973771050572395,\n",
       "  -0.008131963200867176,\n",
       "  -0.03332121670246124,\n",
       "  -0.004409263376146555,\n",
       "  0.012297115288674831,\n",
       "  0.01785065047442913,\n",
       "  0.002847331343218684,\n",
       "  -0.048791781067848206,\n",
       "  -0.1295011341571808,\n",
       "  0.028423728421330452,\n",
       "  -0.0005392383900471032,\n",
       "  -0.10496792942285538,\n",
       "  -0.05855623632669449,\n",
       "  0.004104123916476965,\n",
       "  0.0007337647257372737,\n",
       "  0.030483419075608253,\n",
       "  0.3847196698188782,\n",
       "  -0.07634586095809937,\n",
       "  0.018399901688098907,\n",
       "  0.06829018145799637,\n",
       "  -0.09605786204338074,\n",
       "  0.037226997315883636,\n",
       "  -0.009177065454423428,\n",
       "  0.022580310702323914,\n",
       "  0.003713164245709777,\n",
       "  -0.04076661542057991,\n",
       "  -0.02862207032740116,\n",
       "  -0.06017347797751427,\n",
       "  -0.023617783561348915,\n",
       "  0.002294266363605857,\n",
       "  -0.03286350518465042,\n",
       "  0.034816399216651917,\n",
       "  -0.08446256816387177,\n",
       "  0.017499741166830063,\n",
       "  -0.02129872515797615,\n",
       "  0.027508310973644257,\n",
       "  -0.02065793238580227,\n",
       "  0.012548855505883694,\n",
       "  -0.01807950623333454,\n",
       "  -0.024380633607506752,\n",
       "  -0.011763121001422405,\n",
       "  0.053246814757585526,\n",
       "  -0.007437771186232567,\n",
       "  0.09849897772073746,\n",
       "  0.10002467036247253,\n",
       "  0.08153323084115982,\n",
       "  0.07335549592971802,\n",
       "  0.004317721351981163,\n",
       "  0.10685979574918747,\n",
       "  -0.006255356129258871,\n",
       "  0.0005282724741846323,\n",
       "  -0.016675865277647972,\n",
       "  0.05043953284621239,\n",
       "  -0.035579245537519455,\n",
       "  -0.009672916494309902,\n",
       "  0.021802205592393875,\n",
       "  -0.02232094295322895,\n",
       "  0.0016954303719103336,\n",
       "  -0.051385462284088135,\n",
       "  -0.00441689183935523,\n",
       "  -0.03777625039219856,\n",
       "  -0.04525216296315193,\n",
       "  0.07573558390140533,\n",
       "  -0.10209961980581284,\n",
       "  -0.01742345653474331,\n",
       "  -0.0049470714293420315,\n",
       "  -0.016126614063978195,\n",
       "  -0.0344807431101799,\n",
       "  0.04302464425563812,\n",
       "  0.009451691061258316,\n",
       "  -0.020383307710289955,\n",
       "  0.020795244723558426,\n",
       "  0.03524359315633774,\n",
       "  0.017957448959350586,\n",
       "  -0.016919976100325584,\n",
       "  -0.0003144365327898413,\n",
       "  -0.0045542046427726746,\n",
       "  0.03145986422896385,\n",
       "  -0.05919703096151352,\n",
       "  -0.003591108601540327,\n",
       "  0.07280624657869339,\n",
       "  -0.010054340586066246,\n",
       "  -0.08910068869590759,\n",
       "  0.01965097337961197,\n",
       "  -0.01268616784363985,\n",
       "  0.010283195413649082,\n",
       "  -0.04726608097553253,\n",
       "  0.04116329550743103,\n",
       "  -0.008459988050162792,\n",
       "  -0.039027322083711624,\n",
       "  0.06841223686933517,\n",
       "  0.10112317651510239,\n",
       "  0.02146655134856701,\n",
       "  -0.03380943834781647,\n",
       "  0.02373984083533287,\n",
       "  0.05501662194728851,\n",
       "  0.02332790195941925,\n",
       "  0.021573351696133614,\n",
       "  -0.0749422237277031,\n",
       "  -0.052789103239774704,\n",
       "  0.052606020122766495,\n",
       "  0.021390266716480255,\n",
       "  -0.050805699080228806,\n",
       "  -0.00039358207141049206,\n",
       "  -0.02473154291510582,\n",
       "  0.046137068420648575,\n",
       "  -0.0014475047355517745,\n",
       "  -0.003886712249368429,\n",
       "  -0.018720299005508423,\n",
       "  -0.054009661078453064,\n",
       "  -0.031063184142112732,\n",
       "  -0.048791781067848206,\n",
       "  0.02290070615708828,\n",
       "  0.04424520209431648,\n",
       "  -0.05635923519730568,\n",
       "  0.005660334601998329,\n",
       "  -0.0608752965927124,\n",
       "  0.07768847048282623,\n",
       "  0.020200224593281746,\n",
       "  0.00974920205771923,\n",
       "  0.0015400000847876072,\n",
       "  0.008993981406092644,\n",
       "  -0.02880515344440937,\n",
       "  0.04802893102169037,\n",
       "  -0.032558366656303406,\n",
       "  0.08244864642620087,\n",
       "  -0.003099071327596903,\n",
       "  -0.05825109779834747,\n",
       "  -0.024197548627853394,\n",
       "  0.08958891034126282,\n",
       "  -0.01640123873949051,\n",
       "  0.04519113525748253,\n",
       "  -0.0035987370647490025,\n",
       "  -0.004321535583585501,\n",
       "  0.027264200150966644,\n",
       "  0.02796602062880993,\n",
       "  0.008376074954867363,\n",
       "  -0.0790310874581337,\n",
       "  -0.06444542855024338,\n",
       "  -0.07927519828081131,\n",
       "  -0.3024541139602661,\n",
       "  -8.981347491499037e-05,\n",
       "  0.03957657143473625,\n",
       "  -0.0790921151638031,\n",
       "  -0.04079712927341461,\n",
       "  -0.03059021756052971,\n",
       "  0.044092632830142975,\n",
       "  -0.0034347246401011944,\n",
       "  0.09087049216032028,\n",
       "  0.03997325152158737,\n",
       "  0.05291115865111351,\n",
       "  -0.08189939707517624,\n",
       "  0.01742345653474331,\n",
       "  -0.018491443246603012,\n",
       "  0.013921981677412987,\n",
       "  0.03570130094885826,\n",
       "  0.014852656982839108,\n",
       "  0.029278118163347244,\n",
       "  0.008017536252737045,\n",
       "  -0.0022160743828862906,\n",
       "  -0.009383034892380238,\n",
       "  -0.03771522268652916,\n",
       "  0.03835601359605789,\n",
       "  -0.048334069550037384,\n",
       "  0.012922651134431362,\n",
       "  -0.03948502987623215,\n",
       "  0.15928272902965546,\n",
       "  0.08586620539426804,\n",
       "  0.001425572787411511,\n",
       "  -0.05898343399167061,\n",
       "  0.05285013094544411,\n",
       "  0.03713545575737953,\n",
       "  -0.0013025635853409767,\n",
       "  -0.11369491368532181,\n",
       "  0.03774573653936386,\n",
       "  0.01989508420228958,\n",
       "  -0.035975925624370575,\n",
       "  0.029232347384095192,\n",
       "  0.003072371706366539,\n",
       "  0.002742439741268754,\n",
       "  0.010550192557275295,\n",
       "  -0.005595492664724588,\n",
       "  0.031108954921364784,\n",
       "  -0.0627366453409195,\n",
       "  0.03139883652329445,\n",
       "  -0.08263172954320908,\n",
       "  -0.042505908757448196,\n",
       "  -0.04964616894721985,\n",
       "  -0.0789700597524643,\n",
       "  0.017774365842342377,\n",
       "  0.023068534210324287,\n",
       "  -0.0012739567318931222,\n",
       "  0.04400109127163887,\n",
       "  0.035396162420511246,\n",
       "  -0.05043953284621239,\n",
       "  0.013792297802865505,\n",
       "  -0.01104604359716177,\n",
       "  -0.005141597706824541,\n",
       "  -0.04751019552350044,\n",
       "  -0.009016867727041245,\n",
       "  -0.007163146045058966,\n",
       "  -0.008398960344493389,\n",
       "  -0.007372929248958826,\n",
       "  -0.05825109779834747,\n",
       "  -0.04741865396499634,\n",
       "  0.017606539651751518,\n",
       "  -0.007666625548154116,\n",
       "  -0.009650031104683876,\n",
       "  0.010611220262944698,\n",
       "  -0.004954699892550707,\n",
       "  -0.055474329739809036,\n",
       "  -0.04152946174144745,\n",
       "  -0.0077467248775064945,\n",
       "  0.030697016045451164,\n",
       "  0.03524359315633774,\n",
       "  0.04851715266704559,\n",
       "  0.0021798391826450825,\n",
       "  0.03704391419887543,\n",
       "  -0.03933246061205864,\n",
       "  -0.023800868541002274,\n",
       "  -0.010809561237692833,\n",
       "  0.033778924494981766,\n",
       "  0.06017347797751427,\n",
       "  0.010313709266483784,\n",
       "  -0.001341659459285438,\n",
       "  0.04717453941702843,\n",
       "  0.018918639048933983,\n",
       "  -0.0547419972717762,\n",
       "  0.05132443457841873,\n",
       "  -0.009680544957518578,\n",
       "  -0.02880515344440937,\n",
       "  -0.006842749658972025,\n",
       "  0.014852656982839108,\n",
       "  -0.015569734387099743,\n",
       "  0.021436037495732307,\n",
       "  -0.03798984736204147,\n",
       "  -0.2511906921863556,\n",
       "  0.05053107440471649,\n",
       "  0.031307294964790344,\n",
       "  0.010496793314814568,\n",
       "  0.03426714614033699,\n",
       "  0.02149706520140171,\n",
       "  0.020734217017889023,\n",
       "  -0.04354338347911835,\n",
       "  -0.06804607063531876,\n",
       "  0.00029894118779338896,\n",
       "  -0.03353481367230415,\n",
       "  0.030559703707695007,\n",
       "  0.002185560530051589,\n",
       "  -0.0648726224899292,\n",
       "  -0.004229994025081396,\n",
       "  0.04149894788861275,\n",
       "  0.07030410319566727,\n",
       "  -0.019712001085281372,\n",
       "  -0.015165424905717373,\n",
       "  -0.013105734251439571,\n",
       "  0.0015571641270071268,\n",
       "  -0.0032916904892772436,\n",
       "  0.20944763720035553,\n",
       "  -0.029461203143000603,\n",
       "  0.0030800001695752144,\n",
       "  0.006186699960380793,\n",
       "  -0.008673585951328278,\n",
       "  -0.03695237264037132,\n",
       "  0.0364641509950161,\n",
       "  0.024853598326444626,\n",
       "  -0.00258796289563179,\n",
       "  0.037410080432891846,\n",
       "  0.05904446169734001,\n",
       "  0.019559431821107864,\n",
       "  -0.01908646523952484,\n",
       "  -0.015867244452238083,\n",
       "  0.0048212013207376,\n",
       "  -0.03341275826096535,\n",
       "  0.02352624200284481,\n",
       "  -0.020169710740447044,\n",
       "  0.00628587044775486,\n",
       "  0.01965097337961197,\n",
       "  -0.09575272351503372,\n",
       "  0.0028683096170425415,\n",
       "  0.0182778462767601,\n",
       "  0.014547517523169518,\n",
       "  0.05349092558026314,\n",
       "  0.04720505326986313,\n",
       "  -0.008505758829414845,\n",
       "  0.03408406302332878,\n",
       "  -0.02454845979809761,\n",
       "  0.006304941605776548,\n",
       "  -0.009962799027562141,\n",
       "  0.02151232212781906,\n",
       "  0.021436037495732307,\n",
       "  0.03704391419887543,\n",
       "  -0.017804879695177078,\n",
       "  0.012800594791769981,\n",
       "  -0.007151703350245953,\n",
       "  0.0031734490767121315,\n",
       "  -0.01664535142481327,\n",
       "  -0.037623681128025055,\n",
       "  0.051171865314245224,\n",
       "  0.03707442805171013,\n",
       "  0.0021359752863645554]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = get_embeddings(embedding_client, user_prompt, model_name)\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# embedding_client.embeddings.create(\n",
    "#     input = user_prompt,\n",
    "#     model=\"BAAI/bge-small-en-v1.5\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffee_bot_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
